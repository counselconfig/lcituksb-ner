{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div align=\"center\"><font size=\"5\"><b>Logical Connectives<br>in the UK Statute Book</b></font></div><br>","metadata":{}},{"cell_type":"markdown","source":"<div align=\"center\" style=\"background-color:#000000; color:white;\"><font size=\"5\"><b>Contents</b></font></div>\n\n1. [Hypothesis](#1)\n2. [An overview of data sources](#2)<br>\n    2.1 [The legislation.gov.uk API](#2.1)<br>\n3. [Data quality and pre-processing](#3)<br>\n    3.1 [Wrangling data](#3.1)<br>\n    3.2 [Logical operator definitions](#3.2)<br>\n4. [Results from investigative analysis of data](#4)<br>\n    4.1 [Creating a training set](#4.1)<br>\n    4.2 [Creating a test set](#4.2)<br>\n5. [Model development and testing](#5)<br>\n    5.1 [Training the model and hyperparameter Tuning](#5.1) <br>\n    5.2 [Train and evaluate](#5.2)<br>\n6. [Overview of testing results and final model selection](#6)<br>\n    6.1 [Evaluation scores](#6.1)<br>\n    6.2 [Saving the best model](#6.2)<br>\n    6.3 [Results visualisation](#6.3)<br>\n7. [Hypothesis test and conclusion](#7)<br>\n    7.1 [Analytics and inferences](#7.1)<br>\n    7.2 [Statistcal differences](#7.2)<br>\n    7.3 [Conclusion](#7.3)","metadata":{}},{"cell_type":"markdown","source":"<div align=\"center\" style=\"background-color:#000000; color:white;\"><font size=\"5\"><b>Abstract</b></font></div>","metadata":{}},{"cell_type":"markdown","source":"<font size=\"3\">This demonstration trains a custom Named-Entity Recognition (NER) model using a convolutional neural network to classify text from a set of logical connectives for use on the UK statute book corpus. The model is used to make inferences on three popular statute documents that are obtained from the www.legislation.gov.uk API and pre-processed. The results are compared to establish whether a significant difference exists between them in terms of thier logical connective complexity.</font>","metadata":{}},{"cell_type":"markdown","source":"<a id = \"1\"></a><br><div align=\"center\" style=\"background-color:#000000; color:white;\"><font size=\"5\"><b>1. Hypothesis to test the degree of difference between logical connectives in statutes</b></font></div>","metadata":{}},{"cell_type":"code","source":"<font size=\"3\">Logical connectives are truth functional with symbolic notation supported by context-free language $L_{1}$. The language can be used to teach an NER learning machnine to classify logical connectives</font>\n\n\n| $L_{1}$-connective      | Word                | Term                         |          \n| ----------------------- | ------------------- | ---------------------------- |\n| P$\\wedge$Q              | and                 | Conjunction                  |\n| P$\\vee$Q                | or                  | Disjunction                  |\n| P$\\neg$Q                | not                 | Negation                     |\n| P$\\rightarrow$Q         | if...then           | Material implication         |\n| P$\\leftrightarrow$Q     | if and only if      | Bi-conditional               |\n\n\n<font size=\"3\">The constituent parts of a statement may be expressed formally with $L_{1}$-connective sentence letters as\nwith the legislative statement the ‘[Lords Spiritual] <b>and</b> [Temporal]’ giving:</font><br><br>\n\n$$(\\mathbf P_0 \\wedge \\mathbf Q_0)$$  $\\tag{1.1}$\n\n<font size=\"3\">Let $c$ be the number of occurrences for a type $i$ of logical connective in $L_1$ from a class of statute $j$ divided by the number of its words $w$ such that the quotient $q$ is obtained:</font><br><br>\n\n$$\\begin{equation}\nq_{i,j}=\\frac{c_i}{w_j}\n\\end{equation}$$ $\\tag{1.2}$\n\n<font size=\"3\">Thus, each q for $i$ under $j$ can be tabulated $\\begin{Bmatrix} q_{i,j} \\end{Bmatrix} _{n \\times k}$ to produce a dataset for deriving an overall $L_{1}$-connective index or Freidman $F_r$ statistic:</font><br><br>\n\n$$Q =\n\\begin{pmatrix}\n    q_{11}       & q_{12} & q_{13} & \\dots & q_{1n} \\\\\n    q_{21}       & q_{22} & q_{23} & \\dots & q_{2n} \\\\\n    q_{k1}       & q_{k2} & q_{k3} & \\dots & q_{kn}\n\\end{pmatrix}$$  $\\tag{1.3}$\n\n<font size=\"3\">Now, to calculate an $L_{1}$-connective index for a statute $j$, let all of its column quotients be summed:</font><br><br>\n\n$$\\sum_{j=1}^{k} q_j=q_{1} + q_{2} + \\cdots + q_{k}$$ $\\tag{1.4}$\n\n<font size=\"3\">Furthermore, we obtain $F_r$ after adding rank column $k$ blocks for each treatment row $n$ with the sums named $T$:</font><br><br>\n\n$$F_r = \\frac{12}{nk(k+1)}(T_{1}^{2} + T_{2}^{2}+\\cdots + T_{k}^{2})-3n(k+1)$$ $\\tag{1.5}$\n\n<font size=\"3\">The hypotheses then test whether there is a statistical significance between three statutes formed by a dataset $Q$ on an alpha $α$ significance level of 0.05, using probability value $(p)$ under a Friedman testto conclude either:<font size=\"3\"><br><br>\n\n<font size=\"3\"><b>$H_{0}$: There is no statistically significant difference in a datset of three groups of statute data in $Q$ such that $p$ > $α$</b></font><br><br>\n<font size=\"3\"><b>$H_{1}$: There is a statistically significant difference in a datset of three groups of statute data in $Q$ such that $p$ ≤ $α$</b></font>\n    \n<font size=\"3\">That is, the null hypothesis $H_{0}$ can be rejected if $p$ ≤ α because the comparative results would show more than a 5% chance of a difference being present where none in fact exists.</font>","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"2\"></a><br>\n<div align=\"center\" style=\"background-color:#000000; color:white;\"><font size=\"5\"><b>2. The Data Source</b></font></div>","metadata":{}},{"cell_type":"code","source":"!pip install spacy==2.3.5\n!pip install spacy-lookups-data\n!python -m spacy download en_core_web_sm\n!python -m spacy download en","metadata":{"execution":{"iopub.status.busy":"2021-12-03T13:04:50.307804Z","iopub.execute_input":"2021-12-03T13:04:50.308261Z","iopub.status.idle":"2021-12-03T13:05:21.329909Z","shell.execute_reply.started":"2021-12-03T13:04:50.308221Z","shell.execute_reply":"2021-12-03T13:05:21.328927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"2.1\"></a><br><div align=\"left\"><font size=\"5\"><b>2.1 The legislation.gov.uk API</b></font></div><br>\n<font size=\"3\">The model is trained with Law of Property Act 1925 as it offers a wide range of logical connective terms</font><br>","metadata":{}},{"cell_type":"code","source":"import requests\n\ndata = requests.get(\"https://www.legislation.gov.uk/ukpga/Geo5/15-16/20/enacted/data.html\") #Law of Property Act (1925)\ncontent = data.content\nprint(content) # HTML tags present","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-03T13:11:34.796509Z","iopub.execute_input":"2021-12-03T13:11:34.796905Z","iopub.status.idle":"2021-12-03T13:11:34.928883Z","shell.execute_reply.started":"2021-12-03T13:11:34.79687Z","shell.execute_reply":"2021-12-03T13:11:34.927821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"3\"></a><br>\n<div align=\"center\" style=\"background-color:#000000; color:white;\"><font size=\"5\"><b>3. Data quality and pre-processing</b></font></div>","metadata":{}},{"cell_type":"markdown","source":"<div align=\"left\"><font size=\"5\"><b>3.1 Wrangling data</b></font></div><a id = \"3.1\"></a>","metadata":{}},{"cell_type":"code","source":"!pip install bs4","metadata":{"execution":{"iopub.status.busy":"2021-12-03T13:05:21.518508Z","iopub.execute_input":"2021-12-03T13:05:21.519015Z","iopub.status.idle":"2021-12-03T13:05:27.747176Z","shell.execute_reply.started":"2021-12-03T13:05:21.518965Z","shell.execute_reply":"2021-12-03T13:05:27.745904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nfrom bs4 import BeautifulSoup\n# simplified version\n#soup = BeautifulSoup(content)\n#clean_content = soup.get_text()\n#print(clean_content[500:5000])\n\n\n# verbose version\ndef strip_html_tags(text):\n    soup = BeautifulSoup(text, \"html.parser\")\n    [s.extract() for s in soup(['iframe', 'script'])]\n    stripped_text = soup.get_text()\n    stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n    return stripped_text\n\nclean_content = strip_html_tags(content)\nprint(clean_content[500:5000])","metadata":{"execution":{"iopub.status.busy":"2021-12-03T13:05:27.749307Z","iopub.execute_input":"2021-12-03T13:05:27.749664Z","iopub.status.idle":"2021-12-03T13:05:28.220027Z","shell.execute_reply.started":"2021-12-03T13:05:27.749625Z","shell.execute_reply":"2021-12-03T13:05:28.218972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"3.2\"></a><br>\n\n<div align=\"left\"><font size=\"5\"><b>3.2 Logical operator definitions</b></font></div>","metadata":{}},{"cell_type":"code","source":"#Import the requisite library\nimport spacy\n\n#Build upon the spaCy Small Model\nnlp = spacy.blank(\"en\")\n\n\n#Sample text\ntext = clean_content\n\n#Create the EntityRuler\nruler = nlp.create_pipe(\"entity_ruler\")\n\n#List of Entities and Patterns\nruler.add_patterns([{\"label\": \"(P ∧ Q)\", \"pattern\": \"and\"},{\"label\": \"(P ∧ Q)\", \"pattern\": \"but\"},{\"label\": \"(¬ P)\", \"pattern\": \"not\"},#disregarding material nonimplication\"↛\"\n                    {\"label\": \"(P ∨ Q)\", \"pattern\": \"or\"},{\"label\": \"(P ← Q)\", \"pattern\": \"if\"},{\"label\": \"(P ↓ Q)\", \"pattern\": \"nor\"},#\"neither\" proceeding \"nor\"\n                    {\"label\": \"(P → Q)\", \"pattern\": \"so\"},{\"label\": \"(P → Q)\", \"pattern\": \"therefore\"}])\nnlp.add_pipe(ruler)\n\ndoc = nlp(text)\n\n#extract entities\nfor ent in doc.ents[107:180]:\n   print (ent.text, ent.label_)\n","metadata":{"execution":{"iopub.status.busy":"2021-12-03T13:05:28.221049Z","iopub.execute_input":"2021-12-03T13:05:28.221319Z","iopub.status.idle":"2021-12-03T13:05:38.607308Z","shell.execute_reply.started":"2021-12-03T13:05:28.221291Z","shell.execute_reply":"2021-12-03T13:05:38.606268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"4\"></a><br>\n\n<div align=\"center\" style=\"background-color:#000000; color:white;\"><font size=\"5\"><b>4. Results from investigative analysis of data</b></font></div>","metadata":{}},{"cell_type":"markdown","source":"<div align=\"left\"><font size=\"5\"><b>4.1 Creating a training set</b></font></div><a id = \"4.1\"></a>","metadata":{}},{"cell_type":"code","source":"import spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\ntext = clean_content\ncorpus = []\n\ndoc = nlp(text)\nfor sent in doc.sents:\n    corpus.append(sent.text)\n\nnlp = spacy.blank(\"en\")\n\nruler = nlp.create_pipe(\"entity_ruler\")\n\npatterns = [{\"label\": \"(P ∧ Q)\", \"pattern\": \"and\"},{\"label\": \"(P ∧ Q)\", \"pattern\": \"but\"},{\"label\": \"(¬ P)\", \"pattern\": \"not\"},#disregarding material nonimplication\"↛\"\n                    {\"label\": \"(P ∨ Q)\", \"pattern\": \"or\"},{\"label\": \"(P ← Q)\", \"pattern\": \"if\"},{\"label\": \"(P ↓ Q)\", \"pattern\": \"nor\"},#\"neither\" proceeding \"nor\"\n                    {\"label\": \"(P → Q)\", \"pattern\": \"so\"},{\"label\": \"(P → Q)\", \"pattern\": \"therefore\"}]\n\nruler.add_patterns(patterns)\nnlp.add_pipe(ruler)\n\nTRAIN_DATA = []\nfor sentence in corpus:\n    doc = nlp(sentence)\n    entities = []\n\n    for ent in doc.ents:\n        entities.append([ent.start_char, ent.end_char, ent.label_])\n    TRAIN_DATA.append([sentence, {\"entities\": entities}])\n\nprint (TRAIN_DATA[618:640]) #view sample labelled training data LPA (1925)","metadata":{"execution":{"iopub.status.busy":"2021-12-03T13:05:38.608662Z","iopub.execute_input":"2021-12-03T13:05:38.609007Z","iopub.status.idle":"2021-12-03T13:06:09.858938Z","shell.execute_reply.started":"2021-12-03T13:05:38.608975Z","shell.execute_reply":"2021-12-03T13:06:09.857741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"4.2\"></a><br>\n\n<div align=\"left\"><font size=\"5\"><b>4.2 Creating a test set</b></font></div><br>\n<font size=\"3\">TRAINING DATA is labelled LPA 1925 data and TEST DATA is validation LPA 1925 data.</font><br>","metadata":{}},{"cell_type":"code","source":"import spacy\nimport random\nimport time\nimport warnings\nfrom spacy.util import minibatch, compounding, decaying\nfrom spacy.gold import GoldParse\nfrom spacy.scorer import Scorer\n\n# Test data is validation data\n\nTEST_DATA = TRAIN_DATA\n#TEST_DATA = [['Parliament (Qualification of Women) Act 1918http://www.legislation.gov.uk/ukpga/Geo5/8-9/47/enactedParliament (Qualification of Women)', {'entities': []}], [\"Act 1918Members Of ParliamentQueen's Printer of Acts of Parliament2021-02-05Parliament\", {'entities': []}], ['(Qualification of Women)', {'entities': []}], ['Act', {'entities': []}], ['19181918 Chapter 47An Act to amend the Law with respect to the Capacity of Women to sit in Parliament.[21st November 1918]Be', {'entities': []}], [\"it enacted by the King's most Excellent Majesty, by and with the advice and consent of the Lords Spiritual and Temporal, and Commons, in this present Parliament assembled, and by the authority of the same, as follows:1Capacity of women to be members of Parliament.\", {'entities': [[52, 55, '(P ∧ Q)'], [72, 75, '(P ∧ Q)'], [107, 110, '(P ∧ Q)'], [121, 124, '(P ∧ Q)'], [172, 175, '(P ∧ Q)']]}], ['A woman shall not be disqualified by sex or marriage for being elected to or sitting or voting as a Member of the Commons House of Parliament.2Short title.', {'entities': [[14, 17, '(¬ P)'], [41, 43, '(P ∨ Q)'], [74, 76, '(P ∨ Q)'], [85, 87, '(P ∨ Q)']]}], ['This Act may be cited as the Parliament (Qualification of Women) Act, 1918.', {'entities': []}]]\n\nrandom.seed(0)\n\n# Log files for logging the train and testing scores for references\nfile = open('output_log.txt','w') \nfile.write(\"iteration_no\" + \",\" + \"losses\" +\"\\n\")\n\nfile1 = open('test_output.txt','w')\nfile1.write(\"iteration_no\"+ \",\" +\"ents_p\"+ \",\" +\"ents_r\"+ \",\" +\"ents_f\"+ \",\" +\"ents_per_type\"+ \"\\n\")\n\nfile2 = open('train_output.txt','w')\nfile2.write(\"iteration_no\"+ \",\" +\"ents_p\"+ \",\" +\"ents_r\"+ \",\" +\"ents_f\"+ \",\" +\"ents_per_type\"+ \"\\n\")\n\nmodel = None # (\"en_core_web_sm\")   # Replace with model to train\nstart_training_time = time.time()\n\nprint(\"Test set created\")","metadata":{"execution":{"iopub.status.busy":"2021-12-03T13:06:09.860356Z","iopub.execute_input":"2021-12-03T13:06:09.860688Z","iopub.status.idle":"2021-12-03T13:06:09.87095Z","shell.execute_reply.started":"2021-12-03T13:06:09.860656Z","shell.execute_reply":"2021-12-03T13:06:09.870247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"5\"></a><br>\n<div align=\"center\" style=\"background-color:#000000; color:white;\"><font size=\"5\"><b>5. Model development and testings</b></font></div>","metadata":{}},{"cell_type":"markdown","source":"<font size=\"3\">The training process subscribes to the training pipline taken from spaCy.io below: </font><br><br>\n\n<p><center><img style=\"float: top;margin: max-width:1000px\" src=\"https://spacy.io/training-73950e71e6b59678754a87d6cf1481f9.svg\"></center></p>","metadata":{}},{"cell_type":"markdown","source":"<div align=\"left\"><font size=\"5\"><b>5.1 Training the model and hyperparameter tuning</b></font></div><a id = \"5.1\"></a><br>\n<font size=\"3\">The following hyperparameters are tuned  : </font><br><br>\n\n| Hyperparameter          | Function                                          | Tuned                      |          \n| ----------------------- | --------------------------------------------------|----------------------------|\n| dropout_from            | Initial dropout rate                              | 0.8                        |\n| dropout_decay           | Rate of dropout change 0 = unlimited              | 1e-6                       |\n| token_vector_width      | Width of embedding tables and convolutional layers| 256                        |\n| Conv_depth              |\tDepth of the tok2vec layer                        | 16                         |","metadata":{}},{"cell_type":"code","source":"def train_spacy(TRAIN_DATA, iterations):\n\n    if model is not None:\n        nlp = spacy.load(model)  # load existing spaCy model\n        print(\"Loaded model '%s'\" % model)\n    else:\n        nlp = spacy.blank(\"en\")  # create blank Language class\n        print(\"Created blank 'en' model\")\n\n   # TRAIN_DATA = data\n\n    # create the built-in pipeline components and add them to the pipeline\n    # nlp.create_pipe works for built-ins that are registered with spaCy\n    if 'ner' not in nlp.pipe_names:\n        ner = nlp.create_pipe('ner')\n        nlp.add_pipe(ner, last=True)\n    \n    else:\n        ner = nlp.get_pipe(\"ner\")\n\n    # add labels\n    for _, annotations in TRAIN_DATA:\n         for ent in annotations.get('entities'):\n            ner.add_label(ent[2])\n\n    if model is None:\n        optimizer = nlp.begin_training()\n\n        # For training with customized cfg \n        nlp.entity.cfg['conv_depth'] = 16\n        nlp.entity.cfg['token_vector_width'] = 256\n        # nlp.entity.cfg['bilstm_depth'] = 1\n        # nlp.entity.cfg['beam_width'] = 2\n\n\n    else:\n        print (\"resuming\")\n        optimizer = nlp.resume_training()\n        print (optimizer.learn_rate)\n    \n    # get names of other pipes to disable them during training\n    pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n    \n    dropout = decaying(0.8, 0.2, 1e-6) #minimum, max, decay rate\n    sizes = compounding(1.0, 4.0, 1.001)\n\n    with nlp.disable_pipes(*other_pipes):  # only train NER\n        \n        warnings.filterwarnings(\"once\", category=UserWarning, module='spacy')\n\n        for itn in range(iterations):\n            \n            file = open('outputlog.txt','a') # For logging losses of iterations \n            \n            start = time.time() # Iteration Time\n            \n            if(itn%100 == 0):\n                print(\"Itn  : \"+str(itn), time.time()-start_training_time)\n                print('Testing')\n               \n                results = evaluate(nlp, TEST_DATA)\n                file1 = open('test_output.txt','a') \n                file1.write(str(itn)+','+ str(results['ents_p'])+','+str(results['ents_r'])+','+str(results['ents_f'])+','+str(results[\"ents_per_type\"])+\"\\n\")\n                file1.close()\n\n                results = evaluate(nlp, TRAIN_DATA)\n                file2 = open('train_output.txt','a') \n                file2.write(str(itn)+','+ str(results['ents_p'])+','+str(results['ents_r'])+','+str(results['ents_f'])+','+str(results[\"ents_per_type\"])+\"\\n\")\n                file2.close()\n\n                modelfile = \"training_model\"+str(itn)\n                nlp.to_disk(modelfile)\n  \n            # Reducing Learning rate after certain operations \n            if (itn == 500):\n                optimizer.learn_rate = 0.0001 \n    \n            print(\"Starting iteration \" + str(itn))\n            random.shuffle(TRAIN_DATA)\n            losses = {}\n\n            # use either batches or entire set at once\n\n            ##### For training in Batches\n            batches = minibatch(TRAIN_DATA, size=sizes)\n            for batch in batches:\n                texts, annotations = zip(*batch)\n                nlp.update(texts, annotations, sgd=optimizer, drop=next(dropout), losses=losses)\n\n            ###########################################\n\n            ##### For training in as a single iteration\n            \n            #for text, annotations in TRAIN_DATA:\n                 #nlp.update(\n                         #[text],  # batch of texts\n                         #[annotations],  # batch of annotations\n                         #drop=0.2,  # dropout - make it harder to memorise data\n                         ## drop=next(dropout),  Incase you are using decaying drop\n                         #sgd=optimizer,  # callable to update weights\n                         #losses=losses)\n\n\n            print(\"Losses\",losses)\n            file.write(str(itn) + \",\" + str(losses['ner']) +\"\\n\")\n            print (\"time for iteration:\", time.time()-start)\n            file.close()\n\n    return nlp\n\nprint(\"Training code and hyperparameters set\")","metadata":{"execution":{"iopub.status.busy":"2021-12-03T13:06:09.872024Z","iopub.execute_input":"2021-12-03T13:06:09.872427Z","iopub.status.idle":"2021-12-03T13:06:09.889609Z","shell.execute_reply.started":"2021-12-03T13:06:09.872395Z","shell.execute_reply":"2021-12-03T13:06:09.888789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div align=\"left\"><font size=\"5\"><b>5.2 Train and evaluate</b></font></div><a id = \"5.2\"></a><br>\n<font size=\"4\"> The number of epochs is set to 10 for this demonstration.</font><br>\n<font size=\"4\"> The video will skip the start and resume from 9 iterations to shorten time.</font>","metadata":{}},{"cell_type":"code","source":"def evaluate(ner_model, test_data):\n    scorer = Scorer()\n    for input_, annot in test_data:\n        doc_gold_text = ner_model.make_doc(input_)\n        gold = GoldParse(doc_gold_text, entities=annot['entities'])\n        pred_value = ner_model(input_)\n        scorer.score(pred_value, gold)\n    return scorer.scores\nprint(\"Evaluation class created\")","metadata":{"execution":{"iopub.status.busy":"2021-12-03T13:06:09.891691Z","iopub.execute_input":"2021-12-03T13:06:09.892202Z","iopub.status.idle":"2021-12-03T13:06:09.906579Z","shell.execute_reply.started":"2021-12-03T13:06:09.892167Z","shell.execute_reply":"2021-12-03T13:06:09.905647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prdnlp = train_spacy(TRAIN_DATA, 10) #testing 20 epochs","metadata":{"execution":{"iopub.status.busy":"2021-12-03T13:06:09.908175Z","iopub.execute_input":"2021-12-03T13:06:09.9086Z","iopub.status.idle":"2021-12-03T13:08:58.291957Z","shell.execute_reply.started":"2021-12-03T13:06:09.908567Z","shell.execute_reply":"2021-12-03T13:08:58.290595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"6\"></a><br>\n\n<div align=\"center\" style=\"background-color:#000000; color:white;\"><font size=\"5\"><b>6. Overview of testing results and final model selection</b></font></div>","metadata":{}},{"cell_type":"markdown","source":"<div align=\"left\"><font size=\"5\"><b>6.1 Evaluation scores</b></font></div><a id = \"6.1\"></a>","metadata":{}},{"cell_type":"code","source":"import matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom matplotlib.pyplot import figure\nmatplotlib.style.use('ggplot')\nfigure(num=None, figsize=(9, 7), dpi=80, facecolor='w', edgecolor='k')\n\ndata = np.genfromtxt('./outputlog.txt', delimiter=\",\", names=[\"x\", \"y\"])\nplt.title('Training performance')\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Loss\")\n\nplt.plot(data['x'], data['y'], 'r')\nplt.xticks(range(0,20))\n#plt.savefig('line_plot.pdf') \n# https://stackoverflow.com/questions/52229875/how-to-force-matplotlib-to-show-values-on-x-axis-as-integers\n","metadata":{"execution":{"iopub.status.busy":"2021-12-03T13:08:58.294692Z","iopub.execute_input":"2021-12-03T13:08:58.295001Z","iopub.status.idle":"2021-12-03T13:08:58.574591Z","shell.execute_reply.started":"2021-12-03T13:08:58.294971Z","shell.execute_reply":"2021-12-03T13:08:58.573578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test_text = input(\"Enter your testing text: \")\n# doc = prdnlp(test_text)\n# for ent in doc.ents:\n#     print(ent.text, ent.start_char, ent.end_char, ent.label_)\n\n# Prints Final -- f1 score, precision and recall\nresults = evaluate(prdnlp, TEST_DATA)\nimport json\nprint (json.dumps(results,indent=4))","metadata":{"execution":{"iopub.status.busy":"2021-12-03T13:08:58.575975Z","iopub.execute_input":"2021-12-03T13:08:58.576273Z","iopub.status.idle":"2021-12-03T13:09:04.844992Z","shell.execute_reply.started":"2021-12-03T13:08:58.576242Z","shell.execute_reply":"2021-12-03T13:09:04.843544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div align=\"left\"><font size=\"5\"><b>6.2 Saving the best model and testing a statute</b></font></div><a id = \"6.2\"></a>","metadata":{}},{"cell_type":"code","source":"!pip install bs4","metadata":{"execution":{"iopub.status.busy":"2021-12-03T13:09:04.846786Z","iopub.execute_input":"2021-12-03T13:09:04.847419Z","iopub.status.idle":"2021-12-03T13:09:10.801341Z","shell.execute_reply.started":"2021-12-03T13:09:04.847369Z","shell.execute_reply":"2021-12-03T13:09:10.800277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font size=\"3\">Using the Employment Rights Act (ERA) 1996 as test data</font>","metadata":{}},{"cell_type":"code","source":"import requests\n\nstatute = requests.get(\"https://www.legislation.gov.uk/ukpga/1996/18/enacted/data.html\") #Employment Rights Act 1996\ncontent = statute.content\nfrom bs4 import BeautifulSoup\n\nsoup = BeautifulSoup(content)\nclean_statute = soup.get_text()\nprint(clean_statute[50000:52500])","metadata":{"execution":{"iopub.status.busy":"2021-12-03T13:09:10.803528Z","iopub.execute_input":"2021-12-03T13:09:10.804003Z","iopub.status.idle":"2021-12-03T13:09:11.518236Z","shell.execute_reply.started":"2021-12-03T13:09:10.803954Z","shell.execute_reply":"2021-12-03T13:09:11.516244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font size=\"3\">Make inferences on the ERA 1996</font>","metadata":{}},{"cell_type":"code","source":"#inference - cross-validation on unseen data\n\ntext = clean_statute\ndoc_ERA_data = prdnlp(text)\n\nfor ent in doc_ERA_data.ents:\n    print (ent.text, ent.label_)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-12-03T13:09:11.520536Z","iopub.execute_input":"2021-12-03T13:09:11.521057Z","iopub.status.idle":"2021-12-03T13:09:16.995998Z","shell.execute_reply.started":"2021-12-03T13:09:11.521016Z","shell.execute_reply":"2021-12-03T13:09:16.995034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Count the entities \n\n# Check other text that might have been incorretly counted\n\nfrom collections import Counter \n  \n# Create a list \nz = [(ent.text, ent.label_) for ent in doc_ERA_data.ents]\ncol_count = Counter(z) \nprint(col_count) \n","metadata":{"execution":{"iopub.status.busy":"2021-12-03T13:09:16.997187Z","iopub.execute_input":"2021-12-03T13:09:16.99744Z","iopub.status.idle":"2021-12-03T13:09:17.01813Z","shell.execute_reply.started":"2021-12-03T13:09:16.997414Z","shell.execute_reply":"2021-12-03T13:09:17.017081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save trained Model\n\n# uncomment if model name through command line\n# modelfile = input(\"Enter your Model Name: \")\nmodelfile = \"Final_model\"\nprdnlp.to_disk(modelfile)\nprint(\"Saved model to\", modelfile)","metadata":{"execution":{"iopub.status.busy":"2021-12-03T13:09:17.019518Z","iopub.execute_input":"2021-12-03T13:09:17.019798Z","iopub.status.idle":"2021-12-03T13:09:17.950986Z","shell.execute_reply.started":"2021-12-03T13:09:17.01977Z","shell.execute_reply":"2021-12-03T13:09:17.949992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div align=\"left\"><font size=\"5\"><b>6.3 Results visualisation</b></font></div><a id = \"6.3\"></a>","metadata":{}},{"cell_type":"code","source":"from spacy import displacy\ncolors = {\"(P ∧ Q)\":\"lightblue\", \"(¬ P)\":\"lightgreen\",\"(P ∨ Q)\":\"orange\",\"(P ← Q)\":\"purple\",\"(P ↓ Q)\":\"light red\",\"(P → Q)\":\"grey\"}\noptions = {\"ents\": [\"(P ∧ Q)\", \"(¬ P)\", \"(P ∨ Q)\", \"(P ← Q)\", \"(P ↓ Q)\", \"(P → Q)\"], \"colors\": colors}\ndisplacy.render(doc_ERA_data[15000:20000], style='ent', jupyter=True, options=options)","metadata":{"execution":{"iopub.status.busy":"2021-12-03T13:09:17.952855Z","iopub.execute_input":"2021-12-03T13:09:17.95329Z","iopub.status.idle":"2021-12-03T13:09:18.026847Z","shell.execute_reply.started":"2021-12-03T13:09:17.953244Z","shell.execute_reply":"2021-12-03T13:09:18.025855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install py-readability-metrics\n!python -m nltk.downloader punkt","metadata":{"execution":{"iopub.status.busy":"2021-12-03T13:09:18.028294Z","iopub.execute_input":"2021-12-03T13:09:18.028913Z","iopub.status.idle":"2021-12-03T13:09:26.128639Z","shell.execute_reply.started":"2021-12-03T13:09:18.028854Z","shell.execute_reply":"2021-12-03T13:09:26.127271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font size=\"3\">Show inference figures</font>","metadata":{}},{"cell_type":"code","source":"ERA_NEG_count = 0\n\n# Iterate over all the entities\nfor ent in doc_ERA_data.ents:\n    if (\"(¬ P)\" in ent.label_):  # isues counting (¬ P) when 1\n        # Increment count\n        ERA_NEG_count += 1\n    \nERA_DSJ_count = 0\n\n# Iterate over all the entities\nfor ent in doc_ERA_data.ents:\n    if (\"(P ∨ Q)\" in ent.label_):  \n        # Increment count\n        ERA_DSJ_count += 1\n\nERA_CNJ_count = 0\n\n# Iterate over all the entities\nfor ent in doc_ERA_data.ents:\n    if (\"(P ∧ Q)\" in ent.label_): \n        # Increment count\n        ERA_CNJ_count += 1\n        \nERA_MIMP_count = 0\n\n# Iterate over all the entities\nfor ent in doc_ERA_data.ents:\n    if (\"(P → Q)\" in ent.label_): \n        # Increment count\n        ERA_MIMP_count += 1\n\nERA_JD_count = 0\n\n# Iterate over all the entities\nfor ent in doc_ERA_data.ents:\n    if (\"(P ↓ Q)\" in ent.label_):  \n        # Increment count\n        ERA_JD_count += 1\n\nERA_CIMP_count = 0\n\n# Iterate over all the entities\nfor ent in doc_ERA_data.ents:\n    if (\"(P ← Q)\" in ent.label_):  \n        # Increment count\n        ERA_CIMP_count += 1\n        \n# Print count\nprint(\"(¬ P) =\", ERA_NEG_count)\nprint(\"(P ∨ Q) =\", ERA_DSJ_count)\nprint(\"(P ∧ Q) =\", ERA_CNJ_count)\nprint(\"(P → Q) =\", ERA_MIMP_count)\nprint(\"(P ↓ Q) =\", ERA_JD_count)\nprint(\"(P ← Q) =\", ERA_CIMP_count)\n\n# using split() \n# to count words in string \nres = len(text.split())\ntotal_w = res\n# printing result \nprint (\"Total number of words = \" +  str(total_w)) \n\nprint (\"Total number of logical connectives = \" + str(ERA_NEG_count + ERA_DSJ_count + ERA_CNJ_count + ERA_MIMP_count + ERA_JD_count + ERA_CIMP_count))\n\ntotal_lc = ERA_NEG_count + ERA_DSJ_count + ERA_CNJ_count + ERA_MIMP_count + ERA_JD_count + ERA_CIMP_count\nratio = (total_lc / total_w)\nprint (\"Logical connective count / number of words = \" + str(ratio))\n\n\n\n\n\nfrom readability import Readability\nr = Readability(clean_statute)\n\nf = r.flesch()\n\nprint(\"Flesch score = \" + str(f.score) + \" \" + str(f.ease))\n\n#ERA_lc_quotient = (total_lc / total_w * 100)\n\n#ERA_lc_quotient_rd = str(round(ERA_lc_quotient, 2))\n\n#print (ERA_lc_quotient_rd)\n#print (ERA_lc_quotient)\n\n#print (\"Logical connective quotient = \" + str(round(ERA_lc_quotient, 2)))\n\n###########################\n\nERA_NEG_quotient = (ERA_NEG_count / total_w)\nERA_DSJ_quotient = (ERA_DSJ_count / total_w)\nERA_CNJ_quotient = (ERA_CNJ_count / total_w)\nERA_MIMP_quotient = (ERA_MIMP_count / total_w)\nERA_JD_quotient = (ERA_JD_count / total_w)\nERA_CIMP_quotient = (ERA_CIMP_count / total_w)\n\nERA_total_quotient = ERA_NEG_quotient + ERA_DSJ_quotient + ERA_CNJ_quotient + ERA_MIMP_quotient + ERA_JD_quotient + ERA_CIMP_quotient\n\n\nprint (\"Negation (¬ P) quotient = \" + str(round(ERA_NEG_quotient, 5)))\nprint (\"Disjunction (P ∨ Q) quotient = \" + str(round(ERA_DSJ_quotient, 5)))\nprint (\"Conjunction (P ∨ Q) quotient = \" + str(round(ERA_CNJ_quotient, 5)))\nprint (\"Material Implication (P → Q) quotient = \" + str(round(ERA_MIMP_quotient, 5)))\nprint (\"Joint Denial (P ↓ Q) quotient = \" + str(round(ERA_JD_quotient, 5)))\nprint (\"Converse Implication (P ← Q) quotient = \" + str(round(ERA_CIMP_quotient, 5)))\nprint (\"Total logical connective quotient = \" + str(round(ERA_total_quotient, 2)))","metadata":{"execution":{"iopub.status.busy":"2021-12-03T13:09:26.130342Z","iopub.execute_input":"2021-12-03T13:09:26.130691Z","iopub.status.idle":"2021-12-03T13:09:29.333652Z","shell.execute_reply.started":"2021-12-03T13:09:26.130654Z","shell.execute_reply":"2021-12-03T13:09:29.332727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!/usr/bin/env python\n# a bar plot with errorbars\nimport numpy as np\nimport matplotlib.pyplot as plt\nmatplotlib.style.use('ggplot')\n\ndata = [(\"(¬ P)\", ERA_NEG_count), (\"(P ∨ Q)\", ERA_DSJ_count), (\"(P ∧ Q)\", ERA_CNJ_count), (\"(P → Q)\", ERA_MIMP_count), (\"(P ↓ Q)\", ERA_JD_count), (\"(P ← Q)\", ERA_CIMP_count)]\nnames, values = zip(*data)  \n# names = [x[0] for x in data]  # These two lines are equivalent to the the zip-command.\n# values = [x[1] for x in data] # These two lines are equivalent to the the zip-command.\n\nind = np.arange(len(data))  # the x locations for the groups\nwidth = 0.35       # the width of the bars\n\nfig, ax = plt.subplots(figsize=(8,10))\nrects1 = ax.bar(ind, values, width, color='navy', label=\"Employment Rights Act 1996\")\n\n# add some text for labels, title and axes ticks\nax.set_ylabel('Count')\nax.set_xlabel('Logical connective')\nax.set_title('Occurrences by logical connective in ERA 1996')\nax.legend()\nax.set_xticks(ind)\nax.set_xticklabels(names)\n\n\n\ndef autolabel(rects1):\n    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n    for rect in rects1:\n        height = rect.get_height()\n        ax.annotate('{}'.format(height),\n                    xy=(rect.get_x() + rect.get_width() / 2, height),\n                    xytext=(0, 0),  # 3 points vertical offset\n                    textcoords=\"offset points\",\n                    ha='center', va='bottom')\n\n\nautolabel(rects1)\n\nplt.savefig(\"era.pdf\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-03T13:09:29.337518Z","iopub.execute_input":"2021-12-03T13:09:29.337967Z","iopub.status.idle":"2021-12-03T13:09:29.697878Z","shell.execute_reply.started":"2021-12-03T13:09:29.337922Z","shell.execute_reply":"2021-12-03T13:09:29.696775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"7\"></a><br>\n<div align=\"center\" style=\"background-color:#000000; color:white;\"><font size=\"5\"><b>7. Hypothesis test and conclusion</b></font></div>","metadata":{}},{"cell_type":"markdown","source":"<font size=\"3\">Import Freedom of Information Act 2000 (FIA), Data Protection Act 1998 (DPA), and Health and Safety at Work etc. Act 1974 (HSWA).</font>","metadata":{}},{"cell_type":"code","source":"import requests\n\nstatute = requests.get(\"https://www.legislation.gov.uk/ukpga/2000/36/enacted/data.html\") #Freedom of Information Act 2000\nFIA = statute.content\nfrom bs4 import BeautifulSoup\nsoup1 = BeautifulSoup(FIA)\nclean_FIA = soup1.get_text()\n\nstatute = requests.get(\"https://www.legislation.gov.uk/ukpga/1998/29/enacted/data.html\") #Data Protection Act 1998\nDPA = statute.content\nfrom bs4 import BeautifulSoup\nsoup2 = BeautifulSoup(DPA)\nclean_DPA= soup2.get_text()\n\nstatute = requests.get(\"https://www.legislation.gov.uk/ukpga/1974/37/enacted/data.html\") #Health and Safety at Work etc. Act 1974\nHSWA = statute.content\nfrom bs4 import BeautifulSoup\nsoup3 = BeautifulSoup(HSWA)\nclean_HSWA = soup3.get_text()\n\nprint(\"Statutes imported and cleaned\")","metadata":{"execution":{"iopub.status.busy":"2021-12-03T13:09:29.699174Z","iopub.execute_input":"2021-12-03T13:09:29.699448Z","iopub.status.idle":"2021-12-03T13:09:30.573285Z","shell.execute_reply.started":"2021-12-03T13:09:29.699421Z","shell.execute_reply":"2021-12-03T13:09:30.571437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div align=\"left\"><font size=\"5\"><b>7.1 Analytics and inferences</b></font></div><a id = \"7.1\"></a><br>\n<font size=\"3\">Collecting data from interences of the statutes for statistical testing.</font>","metadata":{}},{"cell_type":"code","source":"FIA_data = clean_FIA\ndoc_FIA_data = prdnlp(FIA_data)\n\nDPA_data = clean_DPA\ndoc_DPA_data = prdnlp(DPA_data)\n\nHSWA_data = clean_HSWA\ndoc_HSWA_data = prdnlp(HSWA_data)\n\nprint(\"Inferences complete\")","metadata":{"execution":{"iopub.status.busy":"2021-12-03T13:09:30.57487Z","iopub.execute_input":"2021-12-03T13:09:30.575283Z","iopub.status.idle":"2021-12-03T13:09:36.681078Z","shell.execute_reply.started":"2021-12-03T13:09:30.575237Z","shell.execute_reply":"2021-12-03T13:09:36.679856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FIA_NEG_count = 0\n\n# Iterate over all the entities\nfor ent in doc_FIA_data.ents:\n    if (\"(¬ P)\" in ent.label_):  # isues counting (¬ P) when 1\n        # Increment count\n        FIA_NEG_count += 1\n    \nFIA_DSJ_count = 0\n\n# Iterate over all the entities\nfor ent in doc_FIA_data.ents:\n    if (\"(P ∨ Q)\" in ent.label_):  \n        # Increment count\n        FIA_DSJ_count += 1\n\nFIA_CNJ_count = 0\n\n# Iterate over all the entities\nfor ent in doc_FIA_data.ents:\n    if (\"(P ∧ Q)\" in ent.label_): \n        # Increment count\n        FIA_CNJ_count += 1\n        \nFIA_MIMP_count = 0\n\n# Iterate over all the entities\nfor ent in doc_FIA_data.ents:\n    if (\"(P → Q)\" in ent.label_): \n        # Increment count\n        FIA_MIMP_count += 1\n\nFIA_JD_count = 0\n\n# Iterate over all the entities\nfor ent in doc_FIA_data.ents:\n    if (\"(P ↓ Q)\" in ent.label_):  \n        # Increment count\n        FIA_JD_count += 1\n\nFIA_CIMP_count = 0\n\n# Iterate over all the entities\nfor ent in doc_FIA_data.ents:\n    if (\"(P ← Q)\" in ent.label_):  \n        # Increment count\n        FIA_CIMP_count += 1\n        \n\n# using split() \n# to count words in string \nFIA_res = len(FIA_data.split()) \nFIA_total_w = FIA_res\n\n#total_lc = FIA_NEG_count + FIA_DSJ_count + FIA_CNJ_count + FIA_MIMP_count + FIA_JD_count + FIA_CIMP_count\n#FIA_lc_quotient = (total_lc / total_w * 100)\n\nFIA_NEG_quotient = (FIA_NEG_count / FIA_total_w)\nFIA_DSJ_quotient = (FIA_DSJ_count / FIA_total_w)\nFIA_CNJ_quotient = (FIA_CNJ_count / FIA_total_w)\nFIA_MIMP_quotient = (FIA_MIMP_count / FIA_total_w)\nFIA_JD_quotient = (FIA_JD_count / FIA_total_w)\nFIA_CIMP_quotient = (FIA_CIMP_count / FIA_total_w)\n\nFIA_total_quotient = FIA_NEG_quotient + FIA_DSJ_quotient + FIA_CNJ_quotient + FIA_MIMP_quotient + FIA_JD_quotient + FIA_CIMP_quotient\nprint(\"Stats complete\")","metadata":{"execution":{"iopub.status.busy":"2021-12-03T13:09:36.682304Z","iopub.execute_input":"2021-12-03T13:09:36.682907Z","iopub.status.idle":"2021-12-03T13:09:36.715796Z","shell.execute_reply.started":"2021-12-03T13:09:36.682862Z","shell.execute_reply":"2021-12-03T13:09:36.714655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FIA_total_quotient = FIA_NEG_quotient + FIA_DSJ_quotient  + FIA_CNJ_quotient  + FIA_MIMP_quotient  + FIA_JD_quotient  + FIA_CIMP_quotient \nprint (FIA_total_quotient)","metadata":{"execution":{"iopub.status.busy":"2021-12-03T13:09:36.71722Z","iopub.execute_input":"2021-12-03T13:09:36.717749Z","iopub.status.idle":"2021-12-03T13:09:36.723497Z","shell.execute_reply.started":"2021-12-03T13:09:36.717714Z","shell.execute_reply":"2021-12-03T13:09:36.722541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DPA_NEG_count = 0\n\n# Iterate over all the entities\nfor ent in doc_DPA_data.ents:\n    if (\"(¬ P)\" in ent.label_):  # isues counting (¬ P) when 1\n        # Increment count\n        DPA_NEG_count += 1\n    \nDPA_DSJ_count = 0\n\n# Iterate over all the entities\nfor ent in doc_DPA_data.ents:\n    if (\"(P ∨ Q)\" in ent.label_):  \n        # Increment count\n        DPA_DSJ_count += 1\n\nDPA_CNJ_count = 0\n\n# Iterate over all the entities\nfor ent in doc_DPA_data.ents:\n    if (\"(P ∧ Q)\" in ent.label_): \n        # Increment count\n        DPA_CNJ_count += 1\n        \nDPA_MIMP_count = 0\n\n# Iterate over all the entities\nfor ent in doc_DPA_data.ents:\n    if (\"(P → Q)\" in ent.label_): \n        # Increment count\n        DPA_MIMP_count += 1\n\nDPA_JD_count = 0\n\n# Iterate over all the entities\nfor ent in doc_DPA_data.ents:\n    if (\"(P ↓ Q)\" in ent.label_):  \n        # Increment count\n        DPA_JD_count += 1\n\nDPA_CIMP_count = 0\n\n# Iterate over all the entities\nfor ent in doc_DPA_data.ents:\n    if (\"(P ← Q)\" in ent.label_):  \n        # Increment count\n        DPA_CIMP_count += 1\n        \n\n# using split() \n# to count words in string \nDPA_res = len(DPA_data.split()) \nDPA_total_w = DPA_res\n\nDPA_NEG_quotient = (DPA_NEG_count / DPA_total_w)\nDPA_DSJ_quotient = (DPA_DSJ_count / DPA_total_w)\nDPA_CNJ_quotient = (DPA_CNJ_count / DPA_total_w)\nDPA_MIMP_quotient = (DPA_MIMP_count / DPA_total_w)\nDPA_JD_quotient = (DPA_JD_count / DPA_total_w)\nDPA_CIMP_quotient = (DPA_CIMP_count / DPA_total_w)\n\nDPA_total_quotient = DPA_NEG_quotient + DPA_DSJ_quotient + DPA_CNJ_quotient + DPA_MIMP_quotient + DPA_JD_quotient + DPA_CIMP_quotient\nprint(\"Stats complete\")","metadata":{"execution":{"iopub.status.busy":"2021-12-03T13:09:36.72462Z","iopub.execute_input":"2021-12-03T13:09:36.725067Z","iopub.status.idle":"2021-12-03T13:09:36.759311Z","shell.execute_reply.started":"2021-12-03T13:09:36.725023Z","shell.execute_reply":"2021-12-03T13:09:36.757523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"HSWA_NEG_count = 0\n\n# Iterate over all the entities\nfor ent in doc_HSWA_data.ents:\n    if (\"(¬ P)\" in ent.label_):  # isues counting (¬ P) when 1\n        # Increment count\n        HSWA_NEG_count += 1\n    \nHSWA_DSJ_count = 0\n\n# Iterate over all the entities\nfor ent in doc_HSWA_data.ents:\n    if (\"(P ∨ Q)\" in ent.label_):  \n        # Increment count\n        HSWA_DSJ_count += 1\n\nHSWA_CNJ_count = 0\n\n# Iterate over all the entities\nfor ent in doc_HSWA_data.ents:\n    if (\"(P ∧ Q)\" in ent.label_): \n        # Increment count\n        HSWA_CNJ_count += 1\n        \nHSWA_MIMP_count = 0\n\n# Iterate over all the entities\nfor ent in doc_HSWA_data.ents:\n    if (\"(P → Q)\" in ent.label_): \n        # Increment count\n        HSWA_MIMP_count += 1\n\nHSWA_JD_count = 0\n\n# Iterate over all the entities\nfor ent in doc_HSWA_data.ents:\n    if (\"(P ↓ Q)\" in ent.label_):  \n        # Increment count\n        HSWA_JD_count += 1\n\nHSWA_CIMP_count = 0\n\n# Iterate over all the entities\nfor ent in doc_HSWA_data.ents:\n    if (\"(P ← Q)\" in ent.label_):  \n        # Increment count\n        HSWA_CIMP_count += 1\n        \n\n# using split() \n# to count words in string \nHSWA_res = len(HSWA_data.split()) \nHSWA_total_w = HSWA_res\n\nHSWA_NEG_quotient = (HSWA_NEG_count / HSWA_total_w)\nHSWA_DSJ_quotient = (HSWA_DSJ_count / HSWA_total_w)\nHSWA_CNJ_quotient = (HSWA_CNJ_count / HSWA_total_w)\nHSWA_MIMP_quotient = (HSWA_MIMP_count / HSWA_total_w)\nHSWA_JD_quotient = (HSWA_JD_count / HSWA_total_w)\nHSWA_CIMP_quotient = (HSWA_CIMP_count / HSWA_total_w)\n\nHSWA_total_quotient = HSWA_NEG_quotient + HSWA_DSJ_quotient + HSWA_CNJ_quotient + HSWA_MIMP_quotient + HSWA_JD_quotient + HSWA_CIMP_quotient\n\nprint(\"Stats complete\")","metadata":{"execution":{"iopub.status.busy":"2021-12-03T13:09:36.766228Z","iopub.execute_input":"2021-12-03T13:09:36.767114Z","iopub.status.idle":"2021-12-03T13:09:36.812688Z","shell.execute_reply.started":"2021-12-03T13:09:36.767058Z","shell.execute_reply":"2021-12-03T13:09:36.811833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#testing stats outputs\n\nprint (FIA_NEG_quotient)\nprint (DPA_NEG_quotient)\nprint (HSWA_NEG_quotient)\n\nprint (HSWA_total_w)\nprint (DPA_total_w)\nprint (FIA_total_w)","metadata":{"execution":{"iopub.status.busy":"2021-12-03T13:09:36.815036Z","iopub.execute_input":"2021-12-03T13:09:36.815427Z","iopub.status.idle":"2021-12-03T13:09:36.822792Z","shell.execute_reply.started":"2021-12-03T13:09:36.815386Z","shell.execute_reply":"2021-12-03T13:09:36.821578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font size=\"3\">Data is used to create a matrix and bar charts</font>","metadata":{}},{"cell_type":"code","source":"# importing package \nimport matplotlib\nimport matplotlib.pyplot as plt \nimport pandas as pd \nimport seaborn as sns\nsns.set_style(\"dark\")\n  \nmatplotlib.style.use('ggplot')\nfont = {'family' : 'normal',\n        'weight' : 'normal',\n        'size'   : 12}                          \nmatplotlib.rc('font', **font)\n\n# create data \ndf = pd.DataFrame([['(¬ P)', FIA_NEG_quotient , DPA_NEG_quotient , HSWA_NEG_quotient ], ['(P ∨ Q)', FIA_DSJ_quotient , DPA_DSJ_quotient , HSWA_DSJ_quotient ], ['(P ∧ Q)', FIA_CNJ_quotient , DPA_CNJ_quotient , HSWA_CNJ_quotient ], \n                   ['(P → Q)', FIA_MIMP_quotient , DPA_MIMP_quotient , HSWA_MIMP_quotient ],['(P ↓ Q)', FIA_JD_quotient , DPA_JD_quotient , HSWA_JD_quotient ],['(P ← Q)', FIA_CIMP_quotient , DPA_CIMP_quotient , HSWA_CIMP_quotient ]], \n                  columns=['Logical connective', 'FIA', 'DPA', 'HSWA']) \n# view data \nprint(df) \n\n# pandas plot grouped bar chart \ndf.plot(x='Logical connective',\n        figsize=(10,8),\n        kind='bar', \n        rot=0,\n        stacked=False, \n        title='Logical connective quotient  type per legislation') \n\nplt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))\n#add df.plot(grid=True)?\n\nplt.savefig(\"fiadpahswa.pdf\")","metadata":{"execution":{"iopub.status.busy":"2021-12-03T13:09:36.824207Z","iopub.execute_input":"2021-12-03T13:09:36.824537Z","iopub.status.idle":"2021-12-03T13:09:37.19079Z","shell.execute_reply.started":"2021-12-03T13:09:36.824496Z","shell.execute_reply":"2021-12-03T13:09:37.189574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div align=\"left\"><font size=\"5\"><b>7.2 Significant difference test</b></font></div><a id = \"7.2\"></a>","metadata":{}},{"cell_type":"code","source":"#!/usr/bin/env python\n# a bar plot with errorbars\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndata = [(\"FIA\", FIA_total_quotient), (\"DPA\", DPA_total_quotient), (\"HSWA\", HSWA_total_quotient)]\nnames, values = zip(*data)\n# names = [x[0] for x in data]  # These two lines are equivalent to the the zip-command.\n# values = [x[1] for x in data] # These two lines are equivalent to the the zip-command.\n\nind = np.arange(len(data))  # the x locations for the groups\nwidth = 0.25    # the width of the bars\n\nfig, ax = plt.subplots(figsize=(6,8))\nrects = ax.bar(ind, values, width, color='navy', label=\" PQ quotient\")\n\n\n# add some text for labels, title and axes ticks\nax.set_ylabel('PQ quotient')\nax.set_xlabel('Legislation')\nax.set_title('Logical connective quotient per legislation')\nax.legend()\n#ax.legend(loc=\"upper center\", bbox_to_anchor=(0.5, 1.15), ncol=2)\nax.set_xticks(ind)\nax.set_xticklabels(names)\n\n\n\n\ndef autolabel(rects):\n    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n    for rect in rects:\n        height = rect.get_height()\n        ax.annotate(f'{rect.get_height():0.3f}',\n                    xy=(rect.get_x() + rect.get_width() / 2, height),\n                    xytext=(0, 0),  # 3 points vertical offset\n                    textcoords=\"offset points\",\n                    ha='center', va='bottom')\n\n\nautolabel(rects)\n\nplt.savefig(\"fiadpahswa_lcq.pdf\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-03T13:09:37.192405Z","iopub.execute_input":"2021-12-03T13:09:37.193041Z","iopub.status.idle":"2021-12-03T13:09:37.476597Z","shell.execute_reply.started":"2021-12-03T13:09:37.192974Z","shell.execute_reply":"2021-12-03T13:09:37.475853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\n\ndf4 = pd.DataFrame({\"legislation\":[\"FIA\", \"DPA\",\"HSWA\"],\n                    \"quotient\":[FIA_total_quotient, DPA_total_quotient, HSWA_total_quotient]})\n\n_, ax = plt.subplots(figsize = (9,9))\nwedges,_,_ = ax.pie(df4[\"quotient\"]\n                    ,labels=df4[\"legislation\"]\n                    ,shadow=False,startangle=90, autopct=\"%1.1f%%\"\n                    ,textprops={'fontsize': 12})\nax.legend(wedges,df4[\"legislation\"], loc='upper right', ncol=1, prop={'size': 10});\n\nplt.title(\"PQ quotient comparison per legislation\")","metadata":{"execution":{"iopub.status.busy":"2021-12-03T13:09:37.47752Z","iopub.execute_input":"2021-12-03T13:09:37.477785Z","iopub.status.idle":"2021-12-03T13:09:37.602969Z","shell.execute_reply.started":"2021-12-03T13:09:37.477742Z","shell.execute_reply":"2021-12-03T13:09:37.602174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Friedman test\nfrom numpy.random import seed\nfrom numpy.random import randn\nfrom scipy.stats import friedmanchisquare\n# seed the random number generator\nseed(1)\n# generate three independent samples\ndata1 = [FIA_NEG_quotient, FIA_DSJ_quotient, FIA_CNJ_quotient, FIA_MIMP_quotient, FIA_JD_quotient, FIA_CIMP_quotient]\ndata2 = [DPA_NEG_quotient, DPA_DSJ_quotient, DPA_CNJ_quotient, DPA_MIMP_quotient, DPA_JD_quotient, DPA_CIMP_quotient]\ndata3 = [HSWA_NEG_quotient, HSWA_DSJ_quotient, HSWA_CNJ_quotient, HSWA_MIMP_quotient, HSWA_JD_quotient, HSWA_CIMP_quotient]\n# compare samples\nstat, p = friedmanchisquare(data1, data2, data3)\nprint('Statistics=%.3f, p=%.3f' % (stat, p))\n# interpret\nalpha = 0.05\nif p > alpha:\n\tprint('No statistically significant difference (accept H0)') \nelse:\n\tprint('Statistically significant difference (Ha holds true reject H0')","metadata":{"execution":{"iopub.status.busy":"2021-12-03T13:09:37.604068Z","iopub.execute_input":"2021-12-03T13:09:37.604462Z","iopub.status.idle":"2021-12-03T13:09:37.615334Z","shell.execute_reply.started":"2021-12-03T13:09:37.604432Z","shell.execute_reply":"2021-12-03T13:09:37.613811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div align=\"left\"><font size=\"5\"><b>7.3 Conclusion</b></font></div><a id = \"7.3\"></a>\n<font size=\"3\">$H_{0}$ holds true: there is no statistically significant difference in NER $L_{1}$ quotients among the groups of statute data $\\chi^2$($2, N = 6)=1.0, p >α$","metadata":{}}]}